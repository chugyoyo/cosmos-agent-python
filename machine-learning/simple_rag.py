import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.chains.retrieval_qa.base import RetrievalQA

# --- é…ç½® ---
# 1. å¾…åŠ è½½æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ (è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„)
LOCAL_FILE_PATH = "./my_local_document.txt"
# 2. æ‚¨çš„ Ollama æ¨¡å‹åç§° (ä¾‹å¦‚ llama2, gemma, mistral ç­‰)
OLLAMA_MODEL = "llama2"
# 3. Chroma æ•°æ®åº“çš„å­˜å‚¨è·¯å¾„
CHROMA_PERSIST_DIR = "./chroma_db"

# ç¡®ä¿ Chroma å­˜å‚¨ç›®å½•å­˜åœ¨
os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)

# å‡è®¾æ‚¨çš„æœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœæ²¡æœ‰ï¼Œè¯·åˆ›å»ºä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ–‡ä»¶ç”¨äºæµ‹è¯•
# ä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º my_local_document.txt çš„æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
# "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨çš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—æ ‡å‡†æ¥å£å’Œå·¥å…·é›†ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…è½»æ¾æ„å»º RAG åº”ç”¨ã€‚"
if not os.path.exists(LOCAL_FILE_PATH):
    print(f"è­¦å‘Šï¼šæ–‡ä»¶ {LOCAL_FILE_PATH} ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºç¤ºä¾‹æ–‡ä»¶...")
    with open(LOCAL_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write("LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨çš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—æ ‡å‡†æ¥å£å’Œå·¥å…·é›†ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…è½»æ¾æ„å»º RAG åº”ç”¨ã€‚")
    print("ç¤ºä¾‹æ–‡ä»¶åˆ›å»ºæˆåŠŸã€‚")

def setup_rag_system(file_path: str):
    """
    æ­å»º RAG ç³»ç»Ÿçš„æ ¸å¿ƒæµç¨‹ï¼šåŠ è½½ -> åˆ†å— -> åµŒå…¥ -> å­˜å‚¨ã€‚

    """
    print("--- æ­¥éª¤ 1: åŠ è½½æ–‡æ¡£ ---")
    try:
        # ä½¿ç”¨ TextLoader ä»æœ¬åœ°è·¯å¾„åŠ è½½æ–‡ä»¶
        # å¦‚æœæ–‡ä»¶æ˜¯ PDF, DOCX ç­‰ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„ loader (å¦‚ PyPDFLoader, UnstructuredFileLoader)
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        print(f"æˆåŠŸåŠ è½½æ–‡ä»¶ï¼š{file_path}ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡ã€‚")
    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

    print("\n--- æ­¥éª¤ 2: æ–‡æ¡£åˆ†å— (Chunking) ---")
    # ä½¿ç”¨ CharacterTextSplitter å°†å¤§æ–‡æ¡£åˆ†å‰²æˆå°å— (chunks)
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(texts)} ä¸ªæ–‡æœ¬å—ã€‚")

    print("\n--- æ­¥éª¤ 3: å‘é‡åµŒå…¥ (Embedding) ---")
    # ä½¿ç”¨ Ollama çš„åµŒå…¥æ¨¡å‹ (é»˜è®¤ä½¿ç”¨ Llama2)
    # ç¡®ä¿æ‚¨çš„ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ
    try:
        embeddings = OllamaEmbeddings(model=OLLAMA_MODEL)
    except Exception as e:
        print(f"Ollama åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œä¸”æ¨¡å‹ {OLLAMA_MODEL} å·²æ‹‰å–ã€‚é”™è¯¯: {e}")
        return None

    print("\n--- æ­¥éª¤ 4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ ---")
    # åˆ›å»ºå¹¶æŒä¹…åŒ– Chroma æ•°æ®åº“
    # å‘é‡æ•°æ®åº“å°†å¤„ç†æ–‡æœ¬å—çš„åµŒå…¥å¹¶å­˜å‚¨èµ·æ¥
    db = Chroma.from_documents(
        texts,
        embeddings,
        persist_directory=CHROMA_PERSIST_DIR  # å°†æ•°æ®åº“å†…å®¹ä¿å­˜åˆ°æœ¬åœ°ç£ç›˜
    )
    print(f"æˆåŠŸå°† {len(texts)} ä¸ªæ–‡æœ¬å—å­˜å‚¨åˆ° Chroma æ•°æ®åº“ ({CHROMA_PERSIST_DIR})ã€‚")
    # å¼ºåˆ¶ä¿å­˜
    db.persist()

    # ä¹Ÿå¯ä»¥é‡æ–°åŠ è½½å·²æœ‰çš„æ•°æ®åº“
    # db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings)

    return db

def run_query(vector_db: Chroma, query: str):
    """
    è¿è¡Œ RAG æ£€ç´¢å’Œç”Ÿæˆæµç¨‹ã€‚
    """
    if vector_db is None:
        print("RAG ç³»ç»ŸæœªæˆåŠŸåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡ŒæŸ¥è¯¢ã€‚")
        return

    print("\n--- æ­¥éª¤ 5: æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ---")

    # åˆå§‹åŒ– Ollama LLM
    try:
        llm = Ollama(model=OLLAMA_MODEL)
    except Exception as e:
        print(f"Ollama LLM åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œã€‚é”™è¯¯: {e}")
        return

    # åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ (Retriever)ï¼Œå®ƒä¼šä»å‘é‡æ•°æ®åº“ä¸­æ‰¾å‡ºä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£å—
    retriever = vector_db.as_retriever()

    # åˆ›å»º RetrievalQA é“¾ï¼Œå®ƒå°†æ•´åˆæ£€ç´¢å™¨å’Œ LLM
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ‰“åŒ…åˆ° LLM çš„ prompt ä¸­
        retriever=retriever
    )

    print(f"\n-> æŸ¥è¯¢: {query}")
    # è¿è¡ŒæŸ¥è¯¢
    result = qa_chain.invoke(query)

    print("\n--- ç»“æœ ---")
    print(result['result'])


if __name__ == "__main__":
    print(f"--- ğŸš€ æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿï¼Œä½¿ç”¨æ¨¡å‹: {OLLAMA_MODEL} ---")

    # 1. è®¾ç½® RAG ç³»ç»Ÿå¹¶è·å–å‘é‡æ•°æ®åº“å®ä¾‹
    chroma_db_instance = setup_rag_system(LOCAL_FILE_PATH)

    if chroma_db_instance:
        # 2. è¿è¡ŒæŸ¥è¯¢
        test_query = "LangChain æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
        run_query(chroma_db_instance, test_query)

    print("\n--- âœ… RAG ç³»ç»Ÿæ‰§è¡Œå®Œæ¯• ---")